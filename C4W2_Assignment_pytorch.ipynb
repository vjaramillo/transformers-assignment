{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05014ac7",
   "metadata": {
    "colab_type": "text",
    "editable": true,
    "id": "7yuytuIllsv1",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n",
    "# Assignment 2: Transformer Summarizer\n",
    "\n",
    "Welcome to the second assignment of course 4. In this assignment you will explore summarization using the transformer model. **Unlike the lecture, you will be implementing an encoder-decoder model. However, don't worry; you will be guided through all the steps, and you will find numerous hints to assist you!**\n",
    "\n",
    "There are many hints in this notebook so feel free to use them as needed. Actually by the end of this notebook you will have implemented the full transformer (both encoder and decoder) but you will only be graded on the implementation of the decoder as the encoder is provided for you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16ee20c6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Check if MPS is available\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS device\")\n",
    "else:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "torch.set_default_device(device)  # Set default device to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5058c3ab",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install -U torchtext torch==2.3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00e9709",
   "metadata": {
    "colab_type": "text",
    "id": "4-3lxSnXRWPx"
   },
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Introduction](#0)\n",
    "- [1 - Importing the Dataset](#1)\n",
    "- [2 - Preprocess the Data](#2)\n",
    "- [3 - Positional Encoding](#3)\n",
    "- [4 - Masking](#4)\n",
    "- [5 - Self-attention](#5)\n",
    "    - [Exercise 1 - scaled_dot_product_attention](#ex-1)\n",
    "- [6 - Encoder](#6)\n",
    "    - [6.1 - Encoder Layer](#6-1)\n",
    "    - [6.2 - Full Encoder](#6-2)\n",
    "- [7 - Decoder](#7)\n",
    "    - [7.1 - Decoder Layer](#7-1)\n",
    "    - [Exercise 2 - DecoderLayer](#ex-2)\n",
    "    - [7.2 - Full Decoder](#7-2)\n",
    "    - [Exercise 3 - Decoder](#ex-3)\n",
    "- [8 - Transformer](#8)\n",
    "    - [Exercise 4 - Transformer](#ex-4)\n",
    "- [9 - Initialize the Model](#9)\n",
    "- [10 - Prepare for Training the Model](#10)\n",
    "- [11 - Summarization](#11)\n",
    "    - [Exercise 5 - next_word](#ex-5)\n",
    "- [12 - Train the Model](#12)\n",
    "- [13 - Summarize some sentences!](#13)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0da363",
   "metadata": {
    "colab_type": "text",
    "id": "H4NlfEQhRWPy"
   },
   "source": [
    "<a name='0'></a>\n",
    "## Introduction\n",
    "\n",
    "Summarization is an important task in natural language processing and could be useful for a consumer enterprise. For example, bots can be used to scrape articles, summarize them, and then you can use sentiment analysis to identify the sentiment about certain stocks. Who wants to read an article or a long email today anyway, when you can build a transformer to summarize text for you? Let's get started. By completing this assignment you will learn to:  \n",
    "\n",
    "- Use built-in functions to preprocess your data\n",
    "- Implement DotProductAttention\n",
    "- Implement Causal Attention\n",
    "- Understand how attention works\n",
    "- Build the transformer model\n",
    "- Evaluate your model\n",
    "- Summarize an article\n",
    "\n",
    "As you can tell, this model is slightly different than the ones you have already implemented. This is heavily based on attention and does not rely on sequences, which allows for parallel computing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b49d856",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "CChWzW-rEHVb",
    "outputId": "a0b3e98b-7fc6-492d-c8ad-3a263b54f670",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import utils\n",
    "\n",
    "import textwrap\n",
    "wrapper = textwrap.TextWrapper(width=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfe093e6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dlai_grader'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mw2_unittest_pytorch\u001b[39;00m\n",
      "File \u001b[0;32m~/w2/w2_unittest_pytorch.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdlai_grader\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgrading\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m test_case, print_feedback\n\u001b[1;32m      5\u001b[0m SEED \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# -\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dlai_grader'"
     ]
    }
   ],
   "source": [
    "import w2_unittest_pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56fc570",
   "metadata": {
    "colab_type": "text",
    "editable": true,
    "id": "kEL2rvaHRWP4",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Import the Dataset\n",
    "You have the dataset saved in a .json file, which you can easily open with pandas. The loading function has already been taken care of in `utils.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "074bcce3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialogue:\n",
      "Lucas: Hey! How was your day?\n",
      "Demi: Hey there! \n",
      "Demi: It was pretty fine, actually, thank you!\n",
      "Demi: I just got promoted! :D\n",
      "Lucas: Whoa! Great news!\n",
      "Lucas: Congratulations!\n",
      "Lucas: Such a success has to be celebrated.\n",
      "Demi: I agree! :D\n",
      "Demi: Tonight at Death & Co.?\n",
      "Lucas: Sure!\n",
      "Lucas: See you there at 10pm?\n",
      "Demi: Yeah! See you there! :D\n",
      "\n",
      "Summary:\n",
      "Demi got promoted. She will celebrate that with Lucas at Death & Co at 10 pm.\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"data/corpus\"\n",
    "\n",
    "train_data, test_data = utils.get_train_test_data(data_dir)\n",
    "\n",
    "# Take one example from the dataset and print it\n",
    "example_summary, example_dialogue = train_data.iloc[10]\n",
    "print(f\"Dialogue:\\n{example_dialogue}\")\n",
    "print(f\"\\nSummary:\\n{example_summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04210324",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Preprocess the data\n",
    "\n",
    "First you will do some preprocessing of the data and split it into inputs and outputs. Here you also remove some of the characters that are specific to this dataset and add the `[EOS]` (end of sentence) token to the end, like it was discussed in the lecture videos. You will also add a `[SOS]` (start of sentence) token to the beginning of the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ba397a0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "document, summary = utils.preprocess(train_data)\n",
    "document_test, summary_test = utils.preprocess(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7f13ac",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b3b1226",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0fe70280",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Now perform the standard preprocessing with the tensorflow library. You will need to modify the filters, because you dont want the `[EOS]` tokens to be removed.\n",
    "\n",
    "Then create the vocabulary by combining the data in the documents and the summaries and using `.fit_on_texts()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5dfab3c8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary: 33746\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import pandas as pd\n",
    "\n",
    "# Get the basic English tokenizer\n",
    "basic_tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# Custom tokenizer function\n",
    "def custom_tokenizer(text):\n",
    "    filters = '!\"#$%&()*+,-./:;<=>?@\\\\^_`{|}~\\t\\n'\n",
    "    return [token for token in text.split() if token not in filters]\n",
    "\n",
    "# Wrapper for the basic English tokenizer that preserves case for special tokens\n",
    "def case_preserving_tokenizer(text):\n",
    "    special_tokens = ['[EOS]', '[SOS]', '[PAD]', '[UNK]']\n",
    "    tokens = basic_tokenizer(text)\n",
    "    result = []\n",
    "    for token in tokens:\n",
    "        if token.upper() in special_tokens:\n",
    "            result.append(token.upper())\n",
    "        else:\n",
    "            result.append(token)\n",
    "\n",
    "    return result  #[token if token.upper() in special_tokens else token.lower() for token in tokens]\n",
    "\n",
    "# Combined tokenizer function\n",
    "def combined_tokenizer(text):\n",
    "    # First, apply the case-preserving basic English tokenizer\n",
    "    tokens = case_preserving_tokenizer(text)\n",
    "    \n",
    "    # Then apply the custom tokenizer\n",
    "    tokens = custom_tokenizer(' '.join(tokens))\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Create a tokenizer\n",
    "# tokenizer = get_tokenizer(combined_tokenizer)\n",
    "\n",
    "# Combine document and summary\n",
    "documents_and_summary = document + summary\n",
    "\n",
    "# Build vocabulary\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield combined_tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(documents_and_summary), specials=['[UNK]', '[PAD]', '[SOS]', '[EOS]'])\n",
    "vocab.set_default_index(vocab['[UNK]'])\n",
    "\n",
    "# Tokenize inputs and targets\n",
    "def tokenize_and_numericalize(text):\n",
    "    return [vocab[token] for token in combined_tokenizer(text)]\n",
    "\n",
    "inputs = [tokenize_and_numericalize(doc) for doc in document]\n",
    "targets = [tokenize_and_numericalize(summ) for summ in summary]\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(f'Size of vocabulary: {vocab_size}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6f70bf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7341b3f5",
   "metadata": {},
   "source": [
    "Now you can pad the tokenized sequences for the training data.\n",
    "\n",
    "For the purpose of this notebook you need to limit the length of the sequences, as transformers are really big models and are not meant to be trained in such small environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c6ddead-2d02-4710-a92b-dc0a6336ceda",
   "metadata": {
    "deletable": false,
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(device)\n",
    "g = torch.Generator(device=device)\n",
    "\n",
    "# Limit the size of the input and output data for being able to run it in this environment.\n",
    "encoder_maxlen = 150\n",
    "decoder_maxlen = 50\n",
    "\n",
    "# Pad the sequences.\n",
    "def pad_sequences(sequences, maxlen, padding='post', truncating='post'):\n",
    "    padded_seqs = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) > maxlen:\n",
    "            if truncating == 'pre':\n",
    "                seq = seq[-maxlen:]\n",
    "            else:  # 'post'\n",
    "                seq = seq[:maxlen]\n",
    "        else:\n",
    "            pad_length = maxlen - len(seq)\n",
    "            if padding == 'pre':\n",
    "                seq = [0] * pad_length + seq\n",
    "            else:  # 'post'\n",
    "                seq = seq + [0] * pad_length\n",
    "        padded_seqs.append(seq)\n",
    "    return np.array(padded_seqs)\n",
    "\n",
    "inputs = pad_sequences(inputs, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
    "targets = pad_sequences(targets, maxlen=decoder_maxlen, padding='post', truncating='post')\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "inputs = torch.tensor(inputs, dtype=torch.int32)\n",
    "targets = torch.tensor(targets, dtype=torch.int32)\n",
    "\n",
    "# Create the final training dataset.\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Create TensorDataset\n",
    "dataset = TensorDataset(inputs, targets)\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, generator=g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cfc353",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58b25fb2",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Positional Encoding\n",
    "\n",
    "In sequence to sequence tasks, the relative order of your data is extremely important to its meaning. When you were training sequential neural networks such as RNNs, you fed your inputs into the network in order. Information about the order of your data was automatically fed into your model. However, when you train a Transformer network using multi-head attention, you feed your data into the model all at once. While this dramatically reduces training time, there is no information about the order of your data. This is where positional encoding is useful.\n",
    "\n",
    "You have learned how to implement the positional encoding in one of this week's labs. Here you will use the `positional_encoding` function to create positional encodings for your transformer. The function is already implemented for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e65672c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def positional_encoding(positions, d_model):\n",
    "    \"\"\"\n",
    "    Precomputes a matrix with all the positional encodings \n",
    "    \n",
    "    Arguments:\n",
    "        positions (int): Maximum number of positions to be encoded \n",
    "        d_model (int): Encoding size \n",
    "    \n",
    "    Returns:\n",
    "        pos_encoding (tf.Tensor): A matrix of shape (1, position, d_model) with the positional encodings\n",
    "    \"\"\"\n",
    "    \n",
    "    position = np.arange(positions)[:, np.newaxis]\n",
    "    k = np.arange(d_model)[np.newaxis, :]\n",
    "    i = k // 2\n",
    "    \n",
    "    # initialize a matrix angle_rads of all the angles \n",
    "    angle_rates = 1 / np.power(10000, (2 * i) / np.float32(d_model))\n",
    "    angle_rads = position * angle_rates\n",
    "  \n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "  \n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "    return torch.tensor(pos_encoding, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1f1063",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<a name='4'></a>\n",
    "## 4 - Masking\n",
    "\n",
    "There are two types of masks that are useful when building your Transformer network: the *padding mask* and the *look-ahead mask*. Both help the softmax computation give the appropriate weights to the words in your input sentence. \n",
    "\n",
    "You have already learned how to implement and use them in one of this week's labs. Here they are implemented for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd96d0b9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfc7471c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def create_padding_mask(decoder_token_ids):\n",
    "    \"\"\"\n",
    "    Creates a matrix mask for the padding cells\n",
    "    \n",
    "    Arguments:\n",
    "        decoder_token_ids (tensor): tensor of size (n, m)\n",
    "    \n",
    "    Returns:\n",
    "        mask (torch.Tensor): binary tensor of size (n, 1, m)\n",
    "    \"\"\"\n",
    "\n",
    "    if torch.is_tensor(decoder_token_ids):\n",
    "        seq = 1 - (decoder_token_ids == 0).to(torch.float)\n",
    "\n",
    "    elif isinstance(decoder_token_ids, np.ndarray):\n",
    "        \n",
    "        seq = 1 - (decoder_token_ids == 0)\n",
    "\n",
    "    else:\n",
    "        seq = torch.logical_not(decoder_token_ids == 0)\n",
    "    \n",
    "    # add extra dimensions to add the padding to the attention logits. \n",
    "    # this will allow for broadcasting later when comparing sequences\n",
    "\n",
    "    if torch.is_tensor(seq):\n",
    "        return seq\n",
    "    else:\n",
    "        return torch.from_numpy(seq).to(torch.float32)\n",
    "\n",
    "\n",
    "def create_look_ahead_mask(batch_size, nhead, sequence_length):\n",
    "    \"\"\"\n",
    "    Returns a lower triangular matrix filled with ones\n",
    "    \n",
    "    Arguments:\n",
    "        sequence_length (int): matrix size\n",
    "    \n",
    "    Returns:\n",
    "        mask (torch.Tensor): binary tensor of size (sequence_length, sequence_length)\n",
    "    \"\"\"\n",
    "    mask = torch.tril(torch.ones(batch_size*nhead, sequence_length, sequence_length), diagonal=0)\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c793d14",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89110af6",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "## 5 - Self-Attention\n",
    "\n",
    "As the authors of the Transformers paper state, \"Attention is All You Need\". \n",
    "\n",
    "<img src=\"images/self-attention.png\" alt=\"Encoder\" width=\"600\"/>\n",
    "<caption><center><font color='purple'><b>Figure 1: Self-Attention calculation visualization</font></center></caption>\n",
    "    \n",
    "The use of self-attention paired with traditional convolutional networks allows for parallelization which speeds up training. You will implement **scaled dot product attention** which takes in a query, key, value, and a mask as inputs to return rich, attention-based vector representations of the words in your sequence. This type of self-attention can be mathematically expressed as:\n",
    "$$\n",
    "\\text { Attention }(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}+{M}\\right) V\\tag{4}\\\n",
    "$$\n",
    "\n",
    "* $Q$ is the matrix of queries \n",
    "* $K$ is the matrix of keys\n",
    "* $V$ is the matrix of values\n",
    "* $M$ is the optional mask you choose to apply \n",
    "* ${d_k}$ is the dimension of the keys, which is used to scale everything down so the softmax doesn't explode\n",
    "\n",
    "<a name='ex-1'></a>\n",
    "### Exercise 1 - scaled_dot_product_attention \n",
    "\n",
    "Implement the function `scaled_dot_product_attention()` to create attention-based representations.\n",
    "\n",
    "**Reminder**: The boolean mask parameter can be passed in as `none` or as either padding or look-ahead. \n",
    "    \n",
    "* Multiply (1. - mask) by -1e9 before adding it to the scaled attention logits. \n",
    "\n",
    "**Additional Hints**\n",
    "* You may find [tf.matmul](https://www.tensorflow.org/api_docs/python/tf/linalg/matmul) useful for matrix multiplication (check how you can use the parameter transpose_b).\n",
    "* You can use [tf.keras.activations.softmax](https://www.tensorflow.org/api_docs/python/tf/keras/activations/softmax) for softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ab3a33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f434073",
   "metadata": {
    "deletable": false,
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: scaled_dot_product_attention\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"\n",
    "    Calculate the attention weights.\n",
    "      q, k, v must have matching leading dimensions.\n",
    "      k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "      The mask has different shapes depending on its type(padding or look ahead) \n",
    "      but it must be broadcastable for addition.\n",
    "\n",
    "    Arguments:\n",
    "        q (tf.Tensor): query of shape (..., seq_len_q, depth)\n",
    "        k (tf.Tensor): key of shape (..., seq_len_k, depth)\n",
    "        v (tf.Tensor): value of shape (..., seq_len_v, depth_v)\n",
    "        mask (tf.Tensor): mask with shape broadcastable \n",
    "              to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        output -- attention_weights\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Multiply q and k transposed.\n",
    "    matmul_qk = torch.linalg.matmul(q, k.T)\n",
    "\n",
    "    # scale matmul_qk with the square root of dk\n",
    "    dk = torch.tensor(k.shape[-1], dtype=torch.float32)\n",
    "    scaled_attention_logits = matmul_qk / np.sqrt(dk)\n",
    "    \n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:  # Don't replace this None\n",
    "        scaled_attention_logits = scaled_attention_logits.unsqueeze(0)\n",
    "        scaled_attention_logits += (1. - mask) * -1e9 \n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores add up to 1.\n",
    "    attention_weights = torch.nn.functional.softmax(scaled_attention_logits, dim=-1)\n",
    "    \n",
    "    # Multiply the attention weights by v\n",
    "    output = torch.linalg.matmul(attention_weights, v)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return output, attention_weights\\\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ed8f5af",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m mask_np \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m]]])\n\u001b[1;32m     11\u001b[0m mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(mask_np)\n\u001b[0;32m---> 13\u001b[0m ou, atw \u001b[38;5;241m=\u001b[39m \u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m ou \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maround(ou, decimals\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     15\u001b[0m atw \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maround(atw, decimals\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[0;32mIn[15], line 27\u001b[0m, in \u001b[0;36mscaled_dot_product_attention\u001b[0;34m(q, k, v, mask)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# scale matmul_qk with the square root of dk\u001b[39;00m\n\u001b[1;32m     26\u001b[0m dk \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(k\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 27\u001b[0m scaled_attention_logits \u001b[38;5;241m=\u001b[39m matmul_qk \u001b[38;5;241m/\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# add the mask to the scaled tensor.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# Don't replace this None\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:1085\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1083\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 1085\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandle_torch_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__array__\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1087\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/overrides.py:1619\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_torch_function_mode_enabled():\n\u001b[1;32m   1616\u001b[0m     \u001b[38;5;66;03m# if we're here, the mode must be set to a TorchFunctionStackMode\u001b[39;00m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;66;03m# this unsets it and calls directly into TorchFunctionStackMode's torch function\u001b[39;00m\n\u001b[1;32m   1618\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _pop_mode_temporarily() \u001b[38;5;28;01mas\u001b[39;00m mode:\n\u001b[0;32m-> 1619\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mmode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpublic_api\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1620\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m   1621\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_device.py:78\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:1087\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   1086\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1087\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "# Test your function!\n",
    "q_np = np.array([[1, 1, 0, 1], [0, 1, 1, 1], [1, 0, 1, 1]]).astype(np.float32)\n",
    "k_np = np.array([[1, 1, 0, 1], [1, 0, 1, 1 ], [1, 1, 1, 0], [0, 0, 0, 1], [0, 1, 0, 1]]).astype(np.float32)\n",
    "v_np = np.array([[0, 0], [1, 0], [1, 0], [1, 1], [1, 1]]).astype(np.float32)\n",
    "\n",
    "q = torch.from_numpy(q_np)\n",
    "k = torch.from_numpy(k_np)\n",
    "v = torch.from_numpy(v_np)\n",
    "\n",
    "mask_np = np.array([[[0, 1, 0, 1, 1], [1, 0, 0, 1, 1], [1, 1, 0, 1, 1]]])\n",
    "mask = torch.from_numpy(mask_np)\n",
    "\n",
    "ou, atw = scaled_dot_product_attention(q, k, v, mask)\n",
    "ou = np.around(ou, decimals=2)\n",
    "atw = np.around(atw, decimals=2)\n",
    "\n",
    "print(f\"Output:\\n {ou}\")\n",
    "print(f\"\\nAttention weigths:\\n {atw}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b970a6e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "##### __Expected Output__\n",
    "\n",
    "```\n",
    "Output:\n",
    " [[[1.   0.62]\n",
    "  [0.62 0.62]\n",
    "  [0.74 0.31]]]\n",
    "\n",
    "Attention weigths:\n",
    " [[[0.   0.38 0.   0.23 0.38]\n",
    "  [0.38 0.   0.   0.23 0.38]\n",
    "  [0.26 0.43 0.   0.16 0.16]]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcbd521",
   "metadata": {},
   "source": [
    "Excellent work! You can now implement self-attention. With that, you can start building the encoder block! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b9c92a",
   "metadata": {},
   "source": [
    "<a name='6'></a>\n",
    "## 6 - Encoder\n",
    "\n",
    "The Transformer Encoder layer pairs self-attention and convolutional neural network style of processing to improve the speed of training and passes K and V matrices to the Decoder, which you'll build later in the assignment. In this section of the assignment, you will implement the Encoder by pairing multi-head attention and a feed forward neural network (Figure 2a). \n",
    "<img src=\"images/encoder_layer.png\" alt=\"Encoder\" width=\"400\"/>\n",
    "<caption><center><font color='purple'><b>Figure 2a: Transformer encoder layer</font></center></caption>\n",
    "\n",
    "* `MultiHeadAttention` you can think of as computing the self-attention several times to detect different features. \n",
    "* Feed forward neural network contains two Dense layers which we'll implement as the function `FullyConnected`\n",
    "\n",
    "Your input sentence first passes through a *multi-head attention layer*, where the encoder looks at other words in the input sentence as it encodes a specific word. The outputs of the multi-head attention layer are then fed to a *feed forward neural network*. The exact same feed forward network is independently applied to each position.\n",
    "   \n",
    "* For the `MultiHeadAttention` layer, you will use the [MultiHeadAttention](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention) implemented in Keras. If you're curious about how to split the query matrix Q, key matrix K, and value matrix V into different heads, you can look through the implementation. \n",
    "* You will also use the [Sequential API](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) with two dense layers to built the feed forward neural network layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3fd59d0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FullyConnected(nn.Module):\n",
    "    \"\"\"\n",
    "    Returns a sequential model consisting of two dense layers. The first dense layer has\n",
    "    fully_connected_dim neurons and is activated by relu. The second dense layer has\n",
    "    embedding_dim and no activation.\n",
    "\n",
    "    Arguments:\n",
    "        embedding_dim (int): output dimension\n",
    "        fully_connected_dim (int): dimension of the hidden layer\n",
    "\n",
    "    Returns:\n",
    "        An instance of nn.Module that can be used as a feed-forward network in a larger model.ÃŸ\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim, fully_connected_dim):\n",
    "        super(FullyConnected, self).__init__()\n",
    "        self.fc1 = nn.Linear(embedding_dim, fully_connected_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(fully_connected_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d7003a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<a name='6-1'></a>\n",
    "### 6.1 Encoder Layer\n",
    "\n",
    "Now you can pair multi-head attention and feed forward neural network together in an encoder layer! You will also use residual connections and layer normalization to help speed up training (Figure 2a).\n",
    "\n",
    "The encoder block (Figure 2) is is already implemented for you. Take a very close look at its implementation, as you will later have to create the decoder yourself, and a lot of the code is very similar. The encoder block performs the following steps: \n",
    "1. It takes the Q, V, K matrices and a boolean mask to a multi-head attention layer. Remember that to compute *self*-attention Q, V and K are the same. You will also perform Dropout in this multi-head attention layer during training. \n",
    "2. There is a skip connection to add your original input `x` and the output of the multi-head attention layer. \n",
    "3. After adding the skip connection, the output passes through the first normalization layer.\n",
    "4. Finally, steps 1-3 are repeated but with the feed forward neural network with a dropout layer instead of the multi-head attention layer. \n",
    "\n",
    "<details>\n",
    "  <summary><font size=\"2\" color=\"darkgreen\"><b>Additional Information (Click to expand)</b></font></summary>\n",
    "    \n",
    "* The `__init__` method creates all the layers that will be accesed by the the `call` method. Wherever you want to use a layer defined inside  the `__init__`  method you will have to use the syntax `self.[insert layer name]`. \n",
    "* You will find the documentation of [MultiHeadAttention](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention) helpful. *Note that if query, key and value are the same, then this function performs self-attention.*\n",
    "* The call arguments for `self.mha` are (Where B is for batch_size, T is for target sequence shapes, and S is output_shape):\n",
    " - `query`: Query Tensor of shape (B, T, dim).\n",
    " - `value`: Value Tensor of shape (B, S, dim).\n",
    " - `key`: Optional key Tensor of shape (B, S, dim). If not given, will use the same value for both key and value, which is the most common case.\n",
    " - `attention_mask`: a boolean mask of shape (B, T, S), that prevents attention to certain positions. The boolean mask specifies which query elements can attend to which key elements, 1 indicates attention and 0 indicates no attention. Broadcasting can happen for the missing batch dimensions and the head dimension.\n",
    " - `return_attention_scores`: A boolean to indicate whether the output should be attention output if True, or (attention_output, attention_scores) if False. Defaults to False.\n",
    " - `training`: Python boolean indicating whether the layer should behave in training mode (adding dropout) or in inference mode (no dropout). Defaults to either using the training mode of the parent layer/model, or False (inference) if there is no parent layer. Take a look at [tf.keras.layers.Dropout](https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/keras/layers/Dropout) for more details (Additional reading in [Keras FAQ](https://keras.io/getting_started/faq/#whats-the-difference-between-the-training-argument-in-call-and-the-trainable-attribute))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4dee76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51c1452b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    The encoder layer is composed by a multi-head self-attention mechanism,\n",
    "    followed by a simple, positionwise fully connected feed-forward network. \n",
    "    This architecture includes a residual connection around each of the two \n",
    "    sub-layers, followed by layer normalization.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim, num_heads, fully_connected_dim,\n",
    "                 dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        \n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = nn.MultiheadAttention(\n",
    "            embed_dim=embedding_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout_rate,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.ffn = FullyConnected(\n",
    "            embedding_dim=embedding_dim,\n",
    "            fully_connected_dim=fully_connected_dim\n",
    "        )\n",
    "\n",
    "        self.layernorm1 = nn.LayerNorm(embedding_dim, eps=layernorm_eps)\n",
    "        self.layernorm2 = nn.LayerNorm(embedding_dim, eps=layernorm_eps)\n",
    "\n",
    "        self.dropout_ffn = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the Encoder Layer\n",
    "        \n",
    "        Arguments:\n",
    "            x (tf.Tensor): Tensor of shape (batch_size, input_seq_len, fully_connected_dim)\n",
    "            training (bool): Boolean, set to true to activate\n",
    "                        the training mode for dropout layers\n",
    "            mask (tf.Tensor): Boolean mask to ensure that the padding is not \n",
    "                    treated as part of the input\n",
    "        Returns:\n",
    "            encoder_layer_out (tf.Tensor): Tensor of shape (batch_size, input_seq_len, embedding_dim)\n",
    "        \"\"\"\n",
    "        # calculate self-attention using mha(~1 line).\n",
    "        # Dropout is added by Keras automatically if the dropout parameter is non-zero during training\n",
    "        self_mha_output, _ = self.mha(x, x, x, key_padding_mask=mask, need_weights=False)  # Self attention (batch_size, input_seq_len, fully_connected_dim)\n",
    "\n",
    "        # skip connection\n",
    "        # apply layer normalization on sum of the input and the attention output to get the  \n",
    "        # output of the multi-head attention layer\n",
    "        skip_x_attention = self.layernorm1(x + self_mha_output)  # (batch_size, input_seq_len, fully_connected_dim)\n",
    "\n",
    "        # pass the output of the multi-head attention layer through a ffn\n",
    "        ffn_output = self.ffn(skip_x_attention)  # (batch_size, input_seq_len, fully_connected_dim)\n",
    "        \n",
    "        # apply dropout layer to ffn output during training\n",
    "        # use `training=training`\n",
    "        ffn_output = self.dropout_ffn(ffn_output)\n",
    "        \n",
    "        # apply layer normalization on sum of the output from multi-head attention (skip connection) and ffn output\n",
    "        # to get the output of the encoder layer\n",
    "        encoder_layer_out = self.layernorm2(skip_x_attention + ffn_output)  # (batch_size, input_seq_len, embedding_dim)\n",
    "        \n",
    "        return encoder_layer_out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a36f99",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e36f13b",
   "metadata": {},
   "source": [
    "<a name='6-2'></a>\n",
    "### 6.2 - Full Encoder\n",
    "\n",
    "Now you're ready to build the full Transformer Encoder (Figure 2b), where you will embed your input and add the positional encodings you calculated. You will then feed your encoded embeddings to a stack of Encoder layers. \n",
    "\n",
    "<img src=\"images/encoder.png\" alt=\"Encoder\" width=\"330\"/>\n",
    "<caption><center><font color='purple'><b>Figure 2b: Transformer Encoder</font></center></caption>\n",
    "\n",
    "The Encoder class is implemented for you. It performs the following steps: \n",
    "1. Pass the input through the Embedding layer.\n",
    "2. Scale the embedding by multiplying it by the square root of the embedding dimension. \n",
    "3. Add the position encoding: self.pos_encoding `[:, :seq_len, :]` to the embedding.\n",
    "4. Pass the encoded embedding through a dropout layer\n",
    "5. Pass the output of the dropout layer through the stack of encoding layers using a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d677d14e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    The entire Encoder starts by passing the input to an embedding layer \n",
    "    and using positional encoding to then pass the output through a stack of\n",
    "    encoder Layers\n",
    "        \n",
    "    \"\"\"  \n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size,\n",
    "               maximum_position_encoding, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_vocab_size, self.embedding_dim)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                                self.embedding_dim)\n",
    "\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(embedding_dim=self.embedding_dim,\n",
    "                                        num_heads=num_heads,\n",
    "                                        fully_connected_dim=fully_connected_dim,\n",
    "                                        dropout_rate=dropout_rate,\n",
    "                                        layernorm_eps=layernorm_eps) \n",
    "                           for _ in range(self.num_layers)]\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the Encoder\n",
    "        \n",
    "        Arguments:\n",
    "            x (tf.Tensor): Tensor of shape (batch_size, seq_len)\n",
    "            mask (tf.Tensor): Boolean mask to ensure that the padding is not \n",
    "                    treated as part of the input\n",
    "\n",
    "        Returns:\n",
    "            x (tf.Tensor): Tensor of shape (batch_size, seq_len, embedding dim)\n",
    "        \"\"\"\n",
    "        seq_len = x.shape[1]\n",
    "        \n",
    "        # Pass input through the Embedding layer\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, embedding_dim)\n",
    "        \n",
    "        # Scale embedding by multiplying it by the square root of the embedding dimension\n",
    "        x *= torch.sqrt(torch.tensor(self.embedding_dim, dtype=torch.float32))\n",
    "        # Add the position encoding to embedding\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        # Pass the encoded embedding through a dropout layer\n",
    "        # use `training=training`\n",
    "        x = self.dropout(x)\n",
    "        # Pass the output through the stack of encoding layers \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9336f81",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 20\u001b[0m\n\u001b[1;32m     16\u001b[0m mask \u001b[38;5;241m=\u001b[39m create_padding_mask(x_)\n\u001b[1;32m     18\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(x_)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mint)\n\u001b[0;32m---> 20\u001b[0m outd \u001b[38;5;241m=\u001b[39m \u001b[43mencoder_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing num_layers=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_layers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, embedding_dim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00memb_d\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and num_heads=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_heads\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx has shape:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[16], line 44\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     41\u001b[0m seq_len \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Pass input through the Embedding layer\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (batch_size, input_seq_len, embedding_dim)\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Scale embedding by multiplying it by the square root of the embedding dimension\u001b[39;00m\n\u001b[1;32m     47\u001b[0m x \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_dim, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:2233\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2163\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Generate a simple lookup table that looks up embeddings in a fixed dictionary and size.\u001b[39;00m\n\u001b[1;32m   2164\u001b[0m \n\u001b[1;32m   2165\u001b[0m \u001b[38;5;124;03mThis module is often used to retrieve word embeddings using indices.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2230\u001b[0m \u001b[38;5;124;03m             [ 0.6262,  0.2438,  0.7471]]])\u001b[39;00m\n\u001b[1;32m   2231\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight):\n\u001b[0;32m-> 2233\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandle_torch_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2234\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2235\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2236\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m padding_idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2245\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m padding_idx \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/overrides.py:1619\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_torch_function_mode_enabled():\n\u001b[1;32m   1616\u001b[0m     \u001b[38;5;66;03m# if we're here, the mode must be set to a TorchFunctionStackMode\u001b[39;00m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;66;03m# this unsets it and calls directly into TorchFunctionStackMode's torch function\u001b[39;00m\n\u001b[1;32m   1618\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _pop_mode_temporarily() \u001b[38;5;28;01mas\u001b[39;00m mode:\n\u001b[0;32m-> 1619\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mmode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpublic_api\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1620\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m   1621\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_device.py:78\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:2264\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2258\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2259\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2260\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2261\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2262\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2263\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "# Test your function!\n",
    "n_layers = 5\n",
    "emb_d = 13\n",
    "n_heads = 13\n",
    "kdim = 9\n",
    "vdim = kdim\n",
    "fully_connected_dim = 16\n",
    "target_vocab_size = 300\n",
    "maximum_position_encoding = 7\n",
    "\n",
    "# encoder_test_output = torch.from_numpy(np.random.rand(3, 7, kdim)).to(torch.float32)\n",
    "encoder_test = Encoder(n_layers, emb_d, n_heads, fully_connected_dim, target_vocab_size, maximum_position_encoding)\n",
    "\n",
    "x_ = np.array([[2, 3, 1, 3, 0, 0, 0]])\n",
    "\n",
    "mask = create_padding_mask(x_)\n",
    "\n",
    "x = torch.from_numpy(x_).to(torch.int)\n",
    "\n",
    "outd = encoder_test(x, mask.to(torch.float32))\n",
    "\n",
    "print(f\"Using num_layers={n_layers}, embedding_dim={emb_d} and num_heads={n_heads}:\\n\")\n",
    "print(f\"x has shape:{x.shape}\")\n",
    "print(f\"Output of decoder has shape:{outd.shape}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3c4707",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c7356fd",
   "metadata": {},
   "source": [
    "<a name='7'></a>\n",
    "## 7 - Decoder\n",
    "\n",
    "Now it is time to implement the decoder. You have seen it in the videos and you can use some help by looking at the encoder implementation above. The Decoder layer takes the K and V matrices generated by the Encoder and computes the second multi-head attention layer with the Q matrix from the output (Figure 3a).\n",
    "\n",
    "<img src=\"images/decoder_layer.png\" alt=\"Decoder\" width=\"250\"/>\n",
    "<caption><center><font color='purple'><b>Figure 3a: Transformer Decoder layer</font></center></caption>\n",
    "\n",
    "<a name='7-1'></a>    \n",
    "### 7.1 - Decoder Layer\n",
    "Again, you'll pair multi-head attention with a feed forward neural network, but this time you'll implement two multi-head attention layers. You will also use residual connections and layer normalization to help speed up training (Figure 3a).\n",
    "\n",
    "<a name='ex-2'></a>    \n",
    "### Exercise 2 - DecoderLayer\n",
    "    \n",
    "Implement `DecoderLayer()` using the `call()` method\n",
    "    \n",
    "1. Block 1 is a multi-head attention layer with a residual connection, and look-ahead mask. Like in the `EncoderLayer`, Dropout is defined within the multi-head attention layer.\n",
    "2. Block 2 will take into account the output of the Encoder, so the multi-head attention layer will receive K and V from the encoder, and Q from the Block 1. You will then apply a normalization layer and a residual connection, just like you did before with the `EncoderLayer`.\n",
    "3. Finally, Block 3 is a feed forward neural network with dropout and normalization layers and a residual connection.\n",
    "    \n",
    "**Additional Hints:**\n",
    "* The first two blocks are fairly similar to the EncoderLayer except you will return `attention_scores` when computing self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8d3a38d",
   "metadata": {
    "deletable": false,
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: DecoderLayer\n",
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    The decoder layer is composed by two multi-head attention blocks, \n",
    "    one that takes the new input and uses self-attention, and the other \n",
    "    one that combines it with the output of the encoder, followed by a\n",
    "    fully connected block. \n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim, num_heads, kdim, vdim, fully_connected_dim, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = nn.MultiheadAttention(\n",
    "            embed_dim=embedding_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout_rate,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.mha2 = nn.MultiheadAttention(\n",
    "            embed_dim=embedding_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout_rate,\n",
    "            kdim=kdim,\n",
    "            vdim=vdim,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.ffn = FullyConnected(\n",
    "            embedding_dim=embedding_dim,\n",
    "            fully_connected_dim=fully_connected_dim\n",
    "        )\n",
    "\n",
    "        self.layernorm1 = nn.LayerNorm(embedding_dim, eps=layernorm_eps)\n",
    "        self.layernorm2 = nn.LayerNorm(embedding_dim, eps=layernorm_eps)\n",
    "        self.layernorm3 = nn.LayerNorm(embedding_dim, eps=layernorm_eps)\n",
    "\n",
    "        self.dropout_ffn = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, x, enc_output, look_ahead_mask, padding_mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the Decoder Layer\n",
    "        \n",
    "        Arguments:\n",
    "            x (tf.Tensor): Tensor of shape (batch_size, target_seq_len, fully_connected_dim)\n",
    "            enc_output (tf.Tensor): Tensor of shape(batch_size, input_seq_len, fully_connected_dim)\n",
    "            training (bool): Boolean, set to true to activate\n",
    "                        the training mode for dropout layers\n",
    "            look_ahead_mask (tf.Tensor): Boolean mask for the target_input\n",
    "            padding_mask (tf.Tensor): Boolean mask for the second multihead attention layer\n",
    "        Returns:\n",
    "            out3 (tf.Tensor): Tensor of shape (batch_size, target_seq_len, fully_connected_dim)\n",
    "            attn_weights_block1 (tf.Tensor): Tensor of shape (batch_size, num_heads, target_seq_len, target_seq_len)\n",
    "            attn_weights_block2 (tf.Tensor): Tensor of shape (batch_size, num_heads, target_seq_len, input_seq_len)\n",
    "        \"\"\"\n",
    "        \n",
    "        ### START CODE HERE ###\n",
    "        # enc_output.shape == (batch_size, input_seq_len, fully_connected_dim)\n",
    "        \n",
    "        # BLOCK 1\n",
    "        # calculate self-attention and return attention scores as attn_weights_block1.\n",
    "        # Dropout will be applied during training (~1 line).\n",
    "        mult_attn_out1, attn_weights_block1 = self.mha1(x, x, x, attn_mask=look_ahead_mask, is_causal=True, need_weights=True, average_attn_weights=False,) \n",
    "        \n",
    "        # apply layer normalization (layernorm1) to the sum of the attention output and the input (~1 line)\n",
    "        Q1 = self.layernorm1(x + mult_attn_out1)\n",
    "\n",
    "        # BLOCK 2\n",
    "        # calculate self-attention using the Q from the first block and K and V from the encoder output. \n",
    "        # Dropout will be applied during training\n",
    "        # Return attention scores as attn_weights_block2 (~1 line)\n",
    "\n",
    "        mult_attn_out2, attn_weights_block2 = self.mha2(Q1, enc_output, enc_output, key_padding_mask=padding_mask, need_weights=True, average_attn_weights=False,)\n",
    "        \n",
    "        # # apply layer normalization (layernorm2) to the sum of the attention output and the Q from the first block (~1 line)\n",
    "        mult_attn_out2 = self.layernorm2(Q1 + mult_attn_out2)\n",
    "                \n",
    "        #BLOCK 3\n",
    "        # pass the output of the second block through a ffn\n",
    "        ffn_output = self.ffn(mult_attn_out2)\n",
    "        \n",
    "        # apply a dropout layer to the ffn output\n",
    "        # use `training=training`\n",
    "        ffn_output = self.dropout_ffn(ffn_output)\n",
    "        \n",
    "        # apply layer normalization (layernorm3) to the sum of the ffn output and the output of the second block\n",
    "        out3 = self.layernorm3(ffn_output + mult_attn_out2)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41686c8b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using embedding_dim=12 and num_heads=12:\n",
      "\n",
      "q has shape:torch.Size([1, 15, 12])\n",
      "Output of encoder has shape:torch.Size([1, 7, 8])\n",
      "\n",
      "Output of decoder layer has shape:torch.Size([1, 15, 12])\n",
      "Att Weights Block 1 has shape:torch.Size([1, 12, 15, 15])\n",
      "Att Weights Block 2 has shape:torch.Size([1, 12, 15, 7])\n"
     ]
    }
   ],
   "source": [
    "# Test your function!\n",
    "key_dim = 12\n",
    "n_heads = 12\n",
    "kdim = 8\n",
    "vdim = 8\n",
    "\n",
    "decoderLayer_test = DecoderLayer(embedding_dim=key_dim, kdim=kdim, vdim=vdim, num_heads=n_heads, fully_connected_dim=32)\n",
    "\n",
    "q_np = np.ones((1, 15, key_dim))\n",
    "q = torch.from_numpy(q_np).to(torch.float32)\n",
    "encoder_test_output = torch.tensor(np.random.rand(1, 7, kdim)).to(torch.float32)\n",
    "look_ahead_mask_ = create_look_ahead_mask(1, n_heads, q.shape[1])\n",
    "\n",
    "look_ahead_mask = look_ahead_mask_.to(torch.float32)\n",
    "\n",
    "print(f\"Using embedding_dim={key_dim} and num_heads={n_heads}:\\n\")\n",
    "print(f\"q has shape:{q.shape}\")\n",
    "print(f\"Output of encoder has shape:{encoder_test_output.shape}\\n\")\n",
    "\n",
    "out, attn_w_b1, attn_w_b2 = decoderLayer_test(q, encoder_test_output, look_ahead_mask, None)\n",
    "\n",
    "print(f\"Output of decoder layer has shape:{out.shape}\")\n",
    "print(f\"Att Weights Block 1 has shape:{attn_w_b1.shape}\")\n",
    "print(f\"Att Weights Block 2 has shape:{attn_w_b2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9b85a3",
   "metadata": {},
   "source": [
    "##### __Expected Output__\n",
    "\n",
    "```\n",
    "Output:\n",
    "Using embedding_dim=12 and num_heads=16:\n",
    "\n",
    "q has shape:(1, 15, 12)\n",
    "Output of encoder has shape:(1, 7, 8)\n",
    "\n",
    "Output of decoder layer has shape:(1, 15, 12)\n",
    "Att Weights Block 1 has shape:(1, 16, 15, 15)\n",
    "Att Weights Block 2 has shape:(1, 16, 15, 7)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ce0ee1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import w2_unittest_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932f7320",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# UNIT TEST\n",
    "w2_unittest_pytorch.test_decoderlayer(DecoderLayer, create_look_ahead_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b82ccf",
   "metadata": {},
   "source": [
    "<a name='7-2'></a> \n",
    "### 7.2 - Full Decoder\n",
    "You're almost there! Time to use your Decoder layer to build a full Transformer Decoder (Figure 3b). You will embed your output and add positional encodings. You will then feed your encoded embeddings to a stack of Decoder layers. \n",
    "\n",
    "\n",
    "<img src=\"images/decoder.png\" alt=\"Decoder\" width=\"300\"/>\n",
    "<caption><center><font color='purple'><b>Figure 3b: Transformer Decoder</font></center></caption>\n",
    "\n",
    "<a name='ex-3'></a>     \n",
    "### Exercise 3 - Decoder\n",
    "\n",
    "Implement `Decoder()` using the `call()` method to embed your output, add positional encoding, and implement multiple decoder layers.\n",
    " \n",
    "In this exercise, you will initialize your Decoder with an Embedding layer, positional encoding, and multiple DecoderLayers. Your `call()` method will perform the following steps: \n",
    "1. Pass your generated output through the Embedding layer.\n",
    "2. Scale your embedding by multiplying it by the square root of your embedding dimension. Remember to cast the embedding dimension to data type `tf.float32` before computing the square root.\n",
    "3. Add the position encoding: self.pos_encoding `[:, :seq_len, :]` to your embedding.\n",
    "4. Pass the encoded embedding through a dropout layer, remembering to use the `training` parameter to set the model training mode. \n",
    "5. Pass the output of the dropout layer through the stack of Decoding layers using a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57dde3be",
   "metadata": {
    "deletable": false,
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: Decoder\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    The entire Encoder starts by passing the target input to an embedding layer \n",
    "    and using positional encoding to then pass the output through a stack of\n",
    "    decoder Layers\n",
    "        \n",
    "    \"\"\" \n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, kdim, vdim, fully_connected_dim, target_vocab_size,\n",
    "               maximum_position_encoding, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(target_vocab_size, self.embedding_dim)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.embedding_dim)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(embedding_dim=self.embedding_dim,\n",
    "                                        num_heads=num_heads,\n",
    "                                        kdim=kdim,\n",
    "                                        vdim=vdim,\n",
    "                                        fully_connected_dim=fully_connected_dim,\n",
    "                                        dropout_rate=dropout_rate,\n",
    "                                        layernorm_eps=layernorm_eps) \n",
    "                           for _ in range(self.num_layers)]\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, x, enc_output, look_ahead_mask, padding_mask):\n",
    "        \"\"\"\n",
    "        Forward  pass for the Decoder\n",
    "        \n",
    "        Arguments:\n",
    "            x (tf.Tensor): Tensor of shape (batch_size, target_seq_len)\n",
    "            enc_output (tf.Tensor):  Tensor of shape(batch_size, input_seq_len, fully_connected_dim)\n",
    "            training (bool): Boolean, set to true to activate\n",
    "                        the training mode for dropout layers\n",
    "            look_ahead_mask (tf.Tensor): Boolean mask for the target_input\n",
    "            padding_mask (tf.Tensor): Boolean mask for the second multihead attention layer\n",
    "        Returns:\n",
    "            x (tf.Tensor): Tensor of shape (batch_size, target_seq_len, fully_connected_dim)\n",
    "            attention_weights (dict[str: tf.Tensor]): Dictionary of tensors containing all the attention weights\n",
    "                                each of shape Tensor of shape (batch_size, num_heads, target_seq_len, input_seq_len)\n",
    "        \"\"\"\n",
    "\n",
    "        seq_len = x.shape[1]\n",
    "        attention_weights = {}\n",
    "        \n",
    "        ### START CODE HERE ###\n",
    "        # create word embeddings \n",
    "        x = self.embedding(x).to(torch.float32)\n",
    "        \n",
    "        # scale embeddings by multiplying by the square root of their dimension\n",
    "        x *= torch.sqrt(torch.tensor(self.embedding_dim).to(torch.float32))\n",
    "        \n",
    "        # add positional encodings to word embedding\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        # apply a dropout layer to x\n",
    "        # use `training=training`\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # use a for loop to pass x through a stack of decoder layers and update attention_weights (~4 lines total)\n",
    "        for i in range(self.num_layers):\n",
    "            # pass x and the encoder output through a stack of decoder layers and save the attention weights\n",
    "            # of block 1 and 2 (~1 line)\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, look_ahead_mask, padding_mask)\n",
    "\n",
    "            #update attention_weights dictionary with the attention weights of block 1 and block 2\n",
    "            attention_weights['decoder_layer{}_block1_self_att'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2_decenc_att'.format(i+1)] = block2\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # x.shape == (batch_size, target_seq_len, fully_connected_dim)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04e877fb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using num_layers=5, embedding_dim=13 and num_heads=13:\n",
      "\n",
      "x has shape:torch.Size([3, 4])\n",
      "Output of encoder has shape:torch.Size([3, 7, 9])\n",
      "\n",
      "Output of decoder has shape:torch.Size([3, 4, 13])\n",
      "\n",
      "Attention weights:\n",
      "decoder_layer1_block1_self_att has shape:torch.Size([3, 13, 4, 4])\n",
      "decoder_layer1_block2_decenc_att has shape:torch.Size([3, 13, 4, 7])\n",
      "decoder_layer2_block1_self_att has shape:torch.Size([3, 13, 4, 4])\n",
      "decoder_layer2_block2_decenc_att has shape:torch.Size([3, 13, 4, 7])\n",
      "decoder_layer3_block1_self_att has shape:torch.Size([3, 13, 4, 4])\n",
      "decoder_layer3_block2_decenc_att has shape:torch.Size([3, 13, 4, 7])\n",
      "decoder_layer4_block1_self_att has shape:torch.Size([3, 13, 4, 4])\n",
      "decoder_layer4_block2_decenc_att has shape:torch.Size([3, 13, 4, 7])\n",
      "decoder_layer5_block1_self_att has shape:torch.Size([3, 13, 4, 4])\n",
      "decoder_layer5_block2_decenc_att has shape:torch.Size([3, 13, 4, 7])\n"
     ]
    }
   ],
   "source": [
    "# Test your function!\n",
    "n_layers = 5\n",
    "emb_d = 13\n",
    "n_heads = 13\n",
    "kdim = 9\n",
    "vdim = kdim\n",
    "fully_connected_dim = 16\n",
    "target_vocab_size = 300\n",
    "maximum_position_encoding = 6\n",
    "\n",
    "x_ = np.array([[3, 2, 1, 1], [2, 1, 1, 0], [2, 1, 1, 0]])\n",
    "x = torch.from_numpy(x_).to(torch.int)\n",
    "\n",
    "encoder_test_output = torch.from_numpy(np.random.rand(3, 7, kdim)).to(torch.float32)\n",
    "\n",
    "look_ahead_mask = create_look_ahead_mask(x.shape[0], n_heads, x.shape[1])\n",
    "\n",
    "decoder_test = Decoder(n_layers, emb_d, n_heads, kdim, vdim, fully_connected_dim, target_vocab_size,maximum_position_encoding)\n",
    "                   \n",
    "outd, att_weights = decoder_test(x, encoder_test_output, look_ahead_mask, None)\n",
    "\n",
    "print(f\"Using num_layers={n_layers}, embedding_dim={emb_d} and num_heads={n_heads}:\\n\")\n",
    "print(f\"x has shape:{x.shape}\")\n",
    "print(f\"Output of encoder has shape:{encoder_test_output.shape}\\n\")\n",
    "\n",
    "print(f\"Output of decoder has shape:{outd.shape}\\n\")\n",
    "print(\"Attention weights:\")\n",
    "for name, tensor in att_weights.items():\n",
    "    print(f\"{name} has shape:{tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94f6e49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c6e15c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9aa2ff15",
   "metadata": {},
   "source": [
    "##### __Expected Output__\n",
    "\n",
    "```\n",
    "Using num_layers=5, embedding_dim=13 and num_heads=17:\n",
    "\n",
    "x has shape:(3, 4)\n",
    "Output of encoder has shape:(3, 7, 9)\n",
    "\n",
    "Output of decoder has shape:(3, 4, 13)\n",
    "\n",
    "Attention weights:\n",
    "decoder_layer1_block1_self_att has shape:(3, 17, 4, 4)\n",
    "decoder_layer1_block2_decenc_att has shape:(3, 17, 4, 7)\n",
    "decoder_layer2_block1_self_att has shape:(3, 17, 4, 4)\n",
    "decoder_layer2_block2_decenc_att has shape:(3, 17, 4, 7)\n",
    "decoder_layer3_block1_self_att has shape:(3, 17, 4, 4)\n",
    "decoder_layer3_block2_decenc_att has shape:(3, 17, 4, 7)\n",
    "decoder_layer4_block1_self_att has shape:(3, 17, 4, 4)\n",
    "decoder_layer4_block2_decenc_att has shape:(3, 17, 4, 7)\n",
    "decoder_layer5_block1_self_att has shape:(3, 17, 4, 4)\n",
    "decoder_layer5_block2_decenc_att has shape:(3, 17, 4, 7)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92745de",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# UNIT TEST\n",
    "w2_unittest_pytorch.test_decoder(Decoder, create_look_ahead_mask, create_padding_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848ba4b5",
   "metadata": {},
   "source": [
    "<a name='8'></a> \n",
    "## 8 - Transformer\n",
    "\n",
    "Phew! This has been quite the assignment! Congratulations! You've done all the hard work, now it's time to put it all together.  \n",
    "\n",
    "<img src=\"images/transformer.png\" alt=\"Transformer\" width=\"550\"/>\n",
    "<caption><center><font color='purple'><b>Figure 4: Transformer</font></center></caption>\n",
    "    \n",
    "The flow of data through the Transformer Architecture is as follows:\n",
    "* First your input passes through an Encoder, which is just repeated Encoder layers that you implemented:\n",
    "    - embedding and positional encoding of your input\n",
    "    - multi-head attention on your input\n",
    "    - feed forward neural network to help detect features\n",
    "* Then the predicted output passes through a Decoder, consisting of the decoder layers that you implemented:\n",
    "    - embedding and positional encoding of the output\n",
    "    - multi-head attention on your generated output\n",
    "    - multi-head attention with the Q from the first multi-head attention layer and the K and V from the Encoder\n",
    "    - a feed forward neural network to help detect features\n",
    "* Finally, after the Nth Decoder layer, one dense layer and a softmax are applied to generate prediction for the next output in your sequence.\n",
    "\n",
    "<a name='ex-4'></a> \n",
    "### Exercise 4 - Transformer\n",
    "\n",
    "Implement `Transformer()` using the `call()` method\n",
    "1. Pass the input through the Encoder with the appropiate mask.\n",
    "2. Pass the encoder output and the target through the Decoder with the appropiate mask.\n",
    "3. Apply a linear transformation and a softmax to get a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9e6cb07",
   "metadata": {
    "deletable": false,
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: Transformer\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete transformer with an Encoder and a Decoder\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, kdim, vdim, fully_connected_dim, input_vocab_size, \n",
    "               target_vocab_size, max_positional_encoding_input,\n",
    "               max_positional_encoding_target, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers=num_layers,\n",
    "                               embedding_dim=embedding_dim,\n",
    "                               num_heads=num_heads,\n",
    "                               fully_connected_dim=fully_connected_dim,\n",
    "                               input_vocab_size=input_vocab_size,\n",
    "                               maximum_position_encoding=max_positional_encoding_input,\n",
    "                               dropout_rate=dropout_rate,\n",
    "                               layernorm_eps=layernorm_eps)\n",
    "\n",
    "        self.decoder = Decoder(num_layers=num_layers, \n",
    "                               embedding_dim=embedding_dim,\n",
    "                               num_heads=num_heads,\n",
    "                               kdim=kdim,\n",
    "                               vdim=vdim,\n",
    "                               fully_connected_dim=fully_connected_dim,\n",
    "                               target_vocab_size=target_vocab_size, \n",
    "                               maximum_position_encoding=max_positional_encoding_target,\n",
    "                               dropout_rate=dropout_rate,\n",
    "                               layernorm_eps=layernorm_eps)\n",
    "        \n",
    "        \n",
    "        self.final_layer = nn.Linear(embedding_dim, target_vocab_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "    def forward(self, input_sentence, output_sentence, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the entire Transformer\n",
    "        Arguments:\n",
    "            input_sentence (tf.Tensor): Tensor of shape (batch_size, input_seq_len)\n",
    "                              An array of the indexes of the words in the input sentence\n",
    "            output_sentence (tf.Tensor): Tensor of shape (batch_size, target_seq_len)\n",
    "                              An array of the indexes of the words in the output sentence\n",
    "            training (bool): Boolean, set to true to activate\n",
    "                        the training mode for dropout layers\n",
    "            enc_padding_mask (tf.Tensor): Boolean mask to ensure that the padding is not \n",
    "                    treated as part of the input\n",
    "            look_ahead_mask (tf.Tensor): Boolean mask for the target_input\n",
    "            dec_padding_mask (tf.Tensor): Boolean mask for the second multihead attention layer\n",
    "        Returns:\n",
    "            final_output (tf.Tensor): The final output of the model\n",
    "            attention_weights (dict[str: tf.Tensor]): Dictionary of tensors containing all the attention weights for the decoder\n",
    "                                each of shape Tensor of shape (batch_size, num_heads, target_seq_len, input_seq_len)\n",
    "        \n",
    "        \"\"\"\n",
    "        ### START CODE HERE ###\n",
    "        # call self.encoder with the appropriate arguments to get the encoder output\n",
    "\n",
    "        enc_output = self.encoder(input_sentence, enc_padding_mask)\n",
    "\n",
    "        # call self.decoder with the appropriate arguments to get the decoder output\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, fully_connected_dim)\n",
    "        dec_output, attention_weights = self.decoder(output_sentence, enc_output, look_ahead_mask, dec_padding_mask)\n",
    "        \n",
    "        # pass decoder output through a linear layer and softmax (~1 line)\n",
    "        final_output = self.final_layer(dec_output)\n",
    "        # final_output = self.softmax(final_output)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3cd93c99",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using num_layers=3, target_vocab_size=350 and num_heads=13:\n",
      "\n",
      "sentence_a has shape:torch.Size([1, 7])\n",
      "sentence_b has shape:torch.Size([1, 1])\n",
      "\n",
      "Output of transformer (summary) has shape:torch.Size([1, 1, 350])\n",
      "\n",
      "Attention weights:\n",
      "decoder_layer1_block1_self_att has shape:torch.Size([1, 13, 1, 1])\n",
      "decoder_layer1_block2_decenc_att has shape:torch.Size([1, 13, 1, 7])\n",
      "decoder_layer2_block1_self_att has shape:torch.Size([1, 13, 1, 1])\n",
      "decoder_layer2_block2_decenc_att has shape:torch.Size([1, 13, 1, 7])\n",
      "decoder_layer3_block1_self_att has shape:torch.Size([1, 13, 1, 1])\n",
      "decoder_layer3_block2_decenc_att has shape:torch.Size([1, 13, 1, 7])\n"
     ]
    }
   ],
   "source": [
    "# Test your function!\n",
    "n_layers = 3\n",
    "emb_d = 13\n",
    "n_heads = 13\n",
    "fully_connected_dim = 8\n",
    "input_vocab_size = 300\n",
    "target_vocab_size = 350\n",
    "kdim = 13\n",
    "vdim = kdim\n",
    "max_positional_encoding_input = 12\n",
    "max_positional_encoding_target = 12\n",
    "\n",
    "transformer = Transformer(n_layers, emb_d, n_heads, kdim, vdim, fully_connected_dim, input_vocab_size, target_vocab_size, max_positional_encoding_input, max_positional_encoding_target)\n",
    "\n",
    "# 0 is the padding value\n",
    "sentence_a_ = np.array([[2, 3, 1, 3, 0, 0, 0]])\n",
    "sentence_b_ = np.array([[2]])\n",
    "\n",
    "sentence_a = torch.from_numpy(sentence_a_).to(torch.int)\n",
    "sentence_b = torch.from_numpy(sentence_b_).to(torch.int)\n",
    "\n",
    "enc_padding_mask = create_padding_mask(sentence_a_)\n",
    "dec_padding_mask = create_padding_mask(sentence_a_)\n",
    "\n",
    "look_ahead_mask = create_look_ahead_mask(1, n_heads, sentence_b.shape[1])\n",
    "\n",
    "test_summary, att_weights = transformer(\n",
    "    sentence_a,\n",
    "    sentence_b,\n",
    "    enc_padding_mask,\n",
    "    look_ahead_mask,\n",
    "    dec_padding_mask\n",
    ")\n",
    "\n",
    "print(f\"Using num_layers={n_layers}, target_vocab_size={target_vocab_size} and num_heads={n_heads}:\\n\")\n",
    "print(f\"sentence_a has shape:{sentence_a.shape}\")\n",
    "print(f\"sentence_b has shape:{sentence_b.shape}\")\n",
    "# print(f\"test_summary: {test_summary}\")\n",
    "\n",
    "print(f\"\\nOutput of transformer (summary) has shape:{test_summary.shape}\\n\")\n",
    "print(\"Attention weights:\")\n",
    "for name, tensor in att_weights.items():\n",
    "    print(f\"{name} has shape:{tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c4e0dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95c9f812",
   "metadata": {},
   "source": [
    "##### __Expected Output__\n",
    "\n",
    "```\n",
    "Using num_layers=3, target_vocab_size=350 and num_heads=17:\n",
    "\n",
    "sentence_a has shape:(1, 7)\n",
    "sentence_b has shape:(1, 7)\n",
    "\n",
    "Output of transformer (summary) has shape:(1, 7, 350)\n",
    "\n",
    "Attention weights:\n",
    "decoder_layer1_block1_self_att has shape:(1, 17, 7, 7)\n",
    "decoder_layer1_block2_decenc_att has shape:(1, 17, 7, 7)\n",
    "decoder_layer2_block1_self_att has shape:(1, 17, 7, 7)\n",
    "decoder_layer2_block2_decenc_att has shape:(1, 17, 7, 7)\n",
    "decoder_layer3_block1_self_att has shape:(1, 17, 7, 7)\n",
    "decoder_layer3_block2_decenc_att has shape:(1, 17, 7, 7)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d035a5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# UNIT TEST\n",
    "w2_unittest.test_transformer(Transformer, create_look_ahead_mask, create_padding_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e8a0c2",
   "metadata": {},
   "source": [
    "<a name='9'></a>\n",
    "## 9 - Initialize the Model\n",
    "Now that you have defined the model, you can initialize and train it. First you can initialize the model with the parameters below. Note that generally these models are much larger and you are using a smaller version to fit this environment and to be able to train it in just a few minutes.\n",
    "\n",
    "The base model described in the original Transformer paper used `num_layers=6`, `embedding_dim=512`, and `fully_connected_dim=2048`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5f79f64",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# Define the model parameters\n",
    "num_layers = 2\n",
    "embedding_dim = 128\n",
    "kdim = vdim = embedding_dim\n",
    "fully_connected_dim = 128\n",
    "num_heads = 2\n",
    "positional_encoding_length = 256\n",
    "\n",
    "# Initialize the model\n",
    "transformer = Transformer(\n",
    "    num_layers, \n",
    "    embedding_dim, \n",
    "    num_heads,\n",
    "    kdim,\n",
    "    vdim,\n",
    "    fully_connected_dim,\n",
    "    vocab_size, \n",
    "    vocab_size, \n",
    "    positional_encoding_length, \n",
    "    positional_encoding_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49ab0cc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71473c27",
   "metadata": {},
   "source": [
    "<a name='10'></a>\n",
    "## 10 - Prepare for Training the Model\n",
    "\n",
    "The original transformer paper uses Adam optimizer with custom learning rate scheduling, which we define in the cell below. This was empirically shown to produce faster convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "eb402089",
   "metadata": {
    "deletable": false,
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "'''A wrapper class for scheduled optimizer '''\n",
    "import numpy as np\n",
    "\n",
    "class ScheduledOptim():\n",
    "    '''A simple wrapper class for learning rate scheduling'''\n",
    "\n",
    "    def __init__(self, optimizer, lr_mul, d_model, n_warmup_steps):\n",
    "        self._optimizer = optimizer\n",
    "        self.lr_mul = lr_mul\n",
    "        self.d_model = d_model\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.n_steps = 0\n",
    "\n",
    "\n",
    "    def step_and_update_lr(self):\n",
    "        \"Step with the inner optimizer\"\n",
    "        self._update_learning_rate()\n",
    "        self._optimizer.step()\n",
    "\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"Zero out the gradients with the inner optimizer\"\n",
    "        self._optimizer.zero_grad()\n",
    "\n",
    "\n",
    "    def _get_lr_scale(self):\n",
    "        d_model = self.d_model\n",
    "        n_steps, n_warmup_steps = self.n_steps, self.n_warmup_steps\n",
    "        return (d_model ** -0.5) * min(n_steps ** (-0.5), n_steps * n_warmup_steps ** (-1.5))\n",
    "\n",
    "\n",
    "    def _update_learning_rate(self):\n",
    "        ''' Learning rate scheduling per step '''\n",
    "\n",
    "        self.n_steps += 1\n",
    "        lr = self.lr_mul * self._get_lr_scale()\n",
    "\n",
    "        for param_group in self._optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = ScheduledOptim(optimizer, lr_mul=1, d_model=embedding_dim, n_warmup_steps=462)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8ed96104-e120-48f1-9703-6b0017db7094",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import math\n",
    "\n",
    "class CustomSchedule(_LRScheduler):\n",
    "    def __init__(self, optimizer, d_model, warmup_steps=2000, last_epoch=-1):\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        super(CustomSchedule, self).__init__(optimizer, last_epoch)\n",
    "    \n",
    "    def get_lr(self):\n",
    "        step = max(1, self.last_epoch)  # To avoid division by zero\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        lr = (self.d_model ** -0.5) * min(arg1, arg2)\n",
    "        return [lr for _ in self.base_lrs]\n",
    "\n",
    "# Example usage\n",
    "# embedding_dim = 512  # Set embedding_dim as needed\n",
    "# model = torch.nn.Linear(embedding_dim, embedding_dim)  # Sample model\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.0002, betas=(0.9, 0.98), eps=1e-9)\n",
    "# scheduler = CustomSchedule(optimizer, d_model=embedding_dim)\n",
    "\n",
    "# # Training loop example\n",
    "# for epoch in range(1000):  # Replace 1000 with your desired number of steps\n",
    "#     optimizer.step()\n",
    "#     scheduler.step()\n",
    "#     print(f\"Learning Rate: {scheduler.get_lr()[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad854ab6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Below you can plot, how the custom learning rate looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d8f4790d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_lr_schedule(sched, num_steps=40000):\n",
    "    lrs = []\n",
    "    for step in range(1, num_steps + 1):\n",
    "        sched.n_steps = step  # Manually update the step count\n",
    "        lr = sched.lr_mul * sched._get_lr_scale()\n",
    "        lrs.append(lr)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, num_steps + 1), lrs)\n",
    "    plt.xlabel('Train Step')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.title('Learning Rate Schedule')\n",
    "    plt.ylim(0, max(lrs) * 1.1)  # Set y-axis limit to slightly above the max lr\n",
    "    plt.show()\n",
    "\n",
    "# Assuming you've already created your ScheduledOptim instance named 'sched'\n",
    "plot_lr_schedule(scheduler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7a0f88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2419b23a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4cfba386",
   "metadata": {},
   "source": [
    "Next, you set up the loss. Since the target sequences are padded, it is important to apply a padding mask when calculating the loss.\n",
    "\n",
    "You will use the sparse categorical cross-entropy loss function (`tf.keras.losses.SparseCategoricalCrossentropy`) and set the parameter `from_logits` to False since the Transformer does not output raw logits since the last layer has a softmax activation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99fc8885",
   "metadata": {
    "deletable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "loss_object = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "def masked_loss(real, pred):\n",
    "    # Create a mask where real != 0\n",
    "    mask = (real != 0).float()\n",
    "    \n",
    "    # Compute the loss\n",
    "    loss_ = loss_object(pred, real)\n",
    "    print(loss_)\n",
    "    # Apply the mask\n",
    "    loss_ *= mask\n",
    "    \n",
    "    # Compute the mean loss over non-padded elements\n",
    "    return loss_.sum() / mask.sum()\n",
    "\n",
    "# Define a metric to track the average loss\n",
    "class AverageMeter:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "train_loss = AverageMeter()\n",
    "\n",
    "# List to store losses for plotting\n",
    "losses = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33db3f0b",
   "metadata": {},
   "source": [
    "Now you can define your custom training function. If you are not very advanced with tensorflow, you can understand this function as an alternative to using `model.compile()` and `model.fit()`, but with added extra flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79092091",
   "metadata": {
    "deletable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def train_step(transformer, inp, tar, optimizer, create_padding_mask, create_look_ahead_mask):\n",
    "    \"\"\"\n",
    "    One training step for the transformer\n",
    "    Arguments:\n",
    "        model (nn.Module): The transformer model\n",
    "        inp (torch.Tensor): Input data to summarize\n",
    "        tar (torch.Tensor): Target (summary)\n",
    "        optimizer (torch.optim.Optimizer): The optimizer\n",
    "        criterion (nn.Module): The loss function\n",
    "        create_padding_mask (function): Function to create padding mask\n",
    "        create_look_ahead_mask (function): Function to create look-ahead mask\n",
    "    Returns:\n",
    "        float: The loss value\n",
    "    \"\"\"\n",
    "    num_heads = 2\n",
    "    transformer.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "\n",
    "    # Create masks\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "    look_ahead_mask = create_look_ahead_mask(tar_inp.shape[0], num_heads, tar_inp.size(1))\n",
    "    dec_padding_mask = create_padding_mask(inp)  # Notice that both encoder and decoder padding masks are equal\n",
    "\n",
    "    print(inp.shape)\n",
    "    print(tar_real.shape)\n",
    "    print(tar_inp.shape)\n",
    "\n",
    "    predictions, _ = transformer(\n",
    "        inp,\n",
    "        tar_inp, \n",
    "        enc_padding_mask, \n",
    "        look_ahead_mask, \n",
    "        dec_padding_mask\n",
    "    )\n",
    "    print(predictions.shape)\n",
    "    \n",
    "    loss = masked_loss(predictions.reshape(-1, predictions.size(-1)), tar_real.reshape(-1))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "# Example usage in a training loop:\n",
    "# for epoch in range(num_epochs):\n",
    "#     for batch in dataloader:\n",
    "#         inp, tar = batch\n",
    "#         loss = train_step(model, inp, tar, optimizer, criterion, create_padding_mask, create_look_ahead_mask)\n",
    "#         train_loss.update(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba53c07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1480d5fd",
   "metadata": {},
   "source": [
    "Now you are ready for training the model. But before starting the training, you can also define one more set of functions to perform the inference. Because you are using a custom training loop, you can do whatever you want between the training steps. And wouldnt't it be fun to see after each epoch some examples of how the model performs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e05c54",
   "metadata": {},
   "source": [
    "<a name='11'></a>\n",
    "## 11 - Summarization\n",
    "\n",
    "The last thing you will implement is inference. With this, you will be able to produce actual summaries of the documents. You will use a simple method called greedy decoding, which means you will predict one word at a time and append it to the output. You will start with an `[SOS]` token and repeat the word by word inference until the model returns you the `[EOS]` token or until you reach the maximum length of the sentence (you need to add this limit, otherwise a poorly trained model could give you infinite sentences without ever producing the `[EOS]` token.\n",
    "\n",
    "<a name='ex-5'></a> \n",
    "### Exercise 5 - next_word\n",
    "Write a helper function that predicts the next word, so you can use it to write the whole sentences. Hint: this is very similar to what happens in the train_step, but you have to set the training of the model to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "175fae70",
   "metadata": {
    "deletable": false,
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: next_word\n",
    "def next_word(transformer, encoder_input, output):\n",
    "    \"\"\"\n",
    "    Helper function for summarization that uses the model to predict just the next word.\n",
    "    Arguments:\n",
    "        encoder_input (tf.Tensor): Input data to summarize\n",
    "        output (tf.Tensor): (incomplete) target (summary)\n",
    "    Returns:\n",
    "        predicted_id (tf.Tensor): The id of the predicted word\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    # Create a padding mask for the input (encoder)\n",
    "    enc_padding_mask = create_padding_mask(encoder_input)\n",
    "    # Create a look-ahead mask for the output\n",
    "    look_ahead_mask = create_look_ahead_mask(encoder_input.shape[0], num_heads, output.shape[1])\n",
    "    # Create a padding mask for the input (decoder)\n",
    "    dec_padding_mask = create_padding_mask(encoder_input)\n",
    "\n",
    "    # Run the prediction of the next word with the transformer model\n",
    "    predictions, attention_weights = transformer(\n",
    "        encoder_input,\n",
    "        output,\n",
    "        enc_padding_mask,\n",
    "        look_ahead_mask,\n",
    "        dec_padding_mask\n",
    "    )\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    predictions = predictions[: ,-1:, :]\n",
    "    predicted_id = torch.argmax(predictions, dim=-1)\n",
    "\n",
    "    return predicted_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0818de8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pad_sequence(sequence, max_length, padding_value=0):\n",
    "    # Convert to tensor if it's a list\n",
    "    if isinstance(sequence, list):\n",
    "        sequence = torch.tensor(sequence)\n",
    "    \n",
    "    # If the sequence is 2D, take the first (and only) row\n",
    "    if sequence.dim() == 2:\n",
    "        sequence = sequence.squeeze(0)\n",
    "    \n",
    "    # Get the current sequence length\n",
    "    current_length = sequence.size(0)\n",
    "    \n",
    "    # If the sequence is already longer than max_length, truncate it\n",
    "    if current_length > max_length:\n",
    "        return sequence[:max_length]\n",
    "    \n",
    "    # Calculate the amount of padding needed\n",
    "    padding_length = max_length - current_length\n",
    "    \n",
    "    # Create a padding tensor\n",
    "    padding = torch.full((padding_length,), padding_value, dtype=sequence.dtype)\n",
    "    \n",
    "    # Concatenate the original sequence with the padding\n",
    "    padded_sequence = torch.cat([sequence, padding])\n",
    "    \n",
    "    return padded_sequence.to(torch.int32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29af50d0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Check if your function works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e97ba77",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted token: tensor([[27038]], device='cuda:0')\n",
      "Predicted word: ['incorporates']\n"
     ]
    }
   ],
   "source": [
    "# Take a random sentence as an input\n",
    "input_document = [\"this is a very long sentence\"]\n",
    "input_document = [tokenize_and_numericalize(doc) for doc in input_document]\n",
    "input_document = pad_sequence(input_document, encoder_maxlen, padding_value=0)\n",
    "encoder_input = input_document.unsqueeze(dim=0)\n",
    "\n",
    "# Take the start of sentence token as the only token in the output to predict the next word\n",
    "output = torch.tensor([[vocab['[SOS]']]])\n",
    "\n",
    "# Predict the next word with your function\n",
    "predicted_token = next_word(transformer, encoder_input, output)\n",
    "print(f\"Predicted token: {predicted_token}\")\n",
    "\n",
    "predicted_word = vocab.lookup_tokens(predicted_token.tolist()[0])\n",
    "print(f\"Predicted word: {predicted_word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fa0072",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7157031c",
   "metadata": {},
   "source": [
    "##### __Expected Output__\n",
    "\n",
    "```\n",
    "Predicted token: [[14859]]\n",
    "Predicted word: masses\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd98959",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Â UNIT TEST\n",
    "w2_unittest.test_next_word(next_word, transformer, encoder_input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6177dc6a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def summarize(model, input_document):\n",
    "    \"\"\"\n",
    "    A function for summarization using the transformer model\n",
    "    Arguments:\n",
    "        input_document (tf.Tensor): Input data to summarize\n",
    "    Returns:\n",
    "        _ (str): The summary of the input_document\n",
    "    \"\"\"    \n",
    "    input_document = [input_document]\n",
    "    input_document = [tokenize_and_numericalize(doc) for doc in input_document]\n",
    "    input_document = pad_sequence(input_document, encoder_maxlen, padding_value=0)\n",
    "    encoder_input = input_document.unsqueeze(dim=0)\n",
    "    \n",
    "    output = torch.tensor([[vocab['[SOS]']]])\n",
    "\n",
    "    for i in range(decoder_maxlen):\n",
    "        predicted_id = next_word(model, encoder_input, output)\n",
    "        output = torch.concat([output, predicted_id], axis=-1)\n",
    "        \n",
    "        if predicted_id.squeeze().item() == vocab['[EOS]']:\n",
    "            break\n",
    "        \n",
    "    output = vocab.lookup_tokens(output.squeeze().tolist())\n",
    "\n",
    "    return \" \".join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d7e91180",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[\"[SOS]\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b15117",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Now you can already summarize a sentence! But beware, since the model was not yet trained at all, it will just produce nonsense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bae4d5f1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set example:\n",
      "[SOS] amanda: i baked  cookies. do you want some?  jerry: sure!  amanda: i'll bring you tomorrow :-) [EOS]\n",
      "\n",
      "Human written summary:\n",
      "[SOS] amanda baked cookies and will bring jerry some tomorrow. [EOS]\n",
      "\n",
      "Model written summary:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[SOS] incorporates movers farty-poop heatwaves airbus dreadfully landmarks acro south weaknesses panini star-wars <link> laps gifted perfected boardgame forrest engelbert favorite frightfully washing-up jannet displays screenshot misunderstandings destroy oooookay seawater parents-in-law godsent achieving turandot condemn fidgety pseudo ellie-mae harmless lookk xiaomi d&d brags dereck comicon containers 15-25 swoop eglinton gimmick dollars'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set_example = 0\n",
    "\n",
    "# Check a summary of a document from the training set\n",
    "print('Training set example:')\n",
    "print(document[training_set_example])\n",
    "print('\\nHuman written summary:')\n",
    "print(summary[training_set_example])\n",
    "print('\\nModel written summary:')\n",
    "summarize(transformer, document[training_set_example])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a014321",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90d6f836",
   "metadata": {},
   "source": [
    "<a name='12'></a>\n",
    "# 12 - Train the model\n",
    "\n",
    "Now you can finally train the model. Below is a loop that will train your model for 20 epochs. note that it should take about 30 seconds per epoch (with the exception of the first few epochs which can take a few minutes each).\n",
    "\n",
    "Note that after each epoch you perform the summarization on one of the sentences in the test set and print it out, so you can see how your model is improving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dfed1160",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Take an example from the test set, to monitor it during training\n",
    "test_example = 0\n",
    "true_summary = summary_test[test_example]\n",
    "true_document = document_test[test_example]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2772af",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6725a879",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS device\")\n",
    "else:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "torch.set_default_device(device)  # Set default device to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "65cc6378",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_n 60: SGD lr 0.0001 -> 0.0001\n",
      "batch_n 120: SGD lr 0.0001 -> 0.0001\n",
      "batch_n 180: SGD lr 0.0002 -> 0.0002\n",
      "Epoch 1/20, Average Loss: 10.2050\n",
      "Time taken for one epoch: 8.374767065048218 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] holidays kurt prefer gus take plans trip leave but her south south suspicion stage take plans match millenials planning and is jessica people new doesn contact and dad conversation is helen will take stays ellie give do semester zack is solve from s day robyn political towel week diva 7\n",
      "\n",
      "batch_n 60: SGD lr 0.0003 -> 0.0003\n",
      "batch_n 120: SGD lr 0.0003 -> 0.0003\n",
      "batch_n 180: SGD lr 0.0004 -> 0.0004\n",
      "Epoch 2/20, Average Loss: 8.2191\n",
      "Time taken for one epoch: 8.18958306312561 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] and is going to buy for a new is going to buy his is going to buy his at 7 [EOS]\n",
      "\n",
      "batch_n 60: SGD lr 0.0005 -> 0.0005\n",
      "batch_n 120: SGD lr 0.0006 -> 0.0006\n",
      "batch_n 180: SGD lr 0.0006 -> 0.0006\n",
      "Epoch 3/20, Average Loss: 6.3726\n",
      "Time taken for one epoch: 8.200421333312988 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] tom is going to go and he will be there [EOS]\n",
      "\n",
      "batch_n 60: SGD lr 0.0007 -> 0.0007\n",
      "batch_n 120: SGD lr 0.0008 -> 0.0008\n",
      "batch_n 180: SGD lr 0.0009 -> 0.0009\n",
      "Epoch 4/20, Average Loss: 5.8785\n",
      "Time taken for one epoch: 8.297457933425903 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] john and she ' s place [EOS]\n",
      "\n",
      "batch_n 60: SGD lr 0.0010 -> 0.0010\n",
      "batch_n 120: SGD lr 0.0010 -> 0.0010\n",
      "batch_n 180: SGD lr 0.0011 -> 0.0011\n",
      "Epoch 5/20, Average Loss: 5.6420\n",
      "Time taken for one epoch: 8.849047183990479 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] they will be there are going to go to the new job [EOS]\n",
      "\n",
      "batch_n 60: SGD lr 0.0012 -> 0.0012\n",
      "batch_n 120: SGD lr 0.0013 -> 0.0013\n",
      "batch_n 180: SGD lr 0.0013 -> 0.0013\n",
      "Epoch 6/20, Average Loss: 5.4703\n",
      "Time taken for one epoch: 8.930886030197144 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] john ' s not the last night [EOS]\n",
      "\n",
      "batch_n 60: SGD lr 0.0014 -> 0.0014\n",
      "batch_n 120: SGD lr 0.0015 -> 0.0015\n",
      "batch_n 180: SGD lr 0.0015 -> 0.0015\n",
      "Epoch 7/20, Average Loss: 5.3287\n",
      "Time taken for one epoch: 8.352876424789429 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] john ' s not going to the last night [EOS]\n",
      "\n",
      "batch_n 60: SGD lr 0.0017 -> 0.0017\n",
      "batch_n 120: SGD lr 0.0017 -> 0.0017\n",
      "batch_n 180: SGD lr 0.0018 -> 0.0018\n",
      "Epoch 8/20, Average Loss: 5.2039\n",
      "Time taken for one epoch: 8.218297004699707 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] john and john and she ' t want to the last night [EOS]\n",
      "\n",
      "batch_n 60: SGD lr 0.0019 -> 0.0019\n",
      "batch_n 120: SGD lr 0.0019 -> 0.0019\n",
      "batch_n 180: SGD lr 0.0020 -> 0.0020\n",
      "Epoch 9/20, Average Loss: 5.0950\n",
      "Time taken for one epoch: 8.384577512741089 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] the new year to go to the beach [EOS]\n",
      "\n",
      "batch_n 60: SGD lr 0.0019 -> 0.0019\n",
      "batch_n 120: SGD lr 0.0019 -> 0.0019\n",
      "batch_n 180: SGD lr 0.0019 -> 0.0019\n",
      "Epoch 10/20, Average Loss: 4.9854\n",
      "Time taken for one epoch: 8.261157512664795 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] emily is in the first [EOS]\n",
      "\n",
      "batch_n 60: SGD lr 0.0018 -> 0.0018\n",
      "batch_n 120: SGD lr 0.0018 -> 0.0018\n",
      "batch_n 180: SGD lr 0.0018 -> 0.0018\n",
      "Epoch 11/20, Average Loss: 4.8959\n",
      "Time taken for one epoch: 8.16457462310791 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] john and john and john and john and john and she ' t want to the party tonight [EOS]\n",
      "\n",
      "batch_n 60: SGD lr 0.0017 -> 0.0017\n",
      "batch_n 120: SGD lr 0.0017 -> 0.0017\n",
      "batch_n 180: SGD lr 0.0017 -> 0.0017\n",
      "Epoch 12/20, Average Loss: 4.8168\n",
      "Time taken for one epoch: 8.16928482055664 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] mike and the restaurant [EOS]\n",
      "\n",
      "batch_n 60: SGD lr 0.0017 -> 0.0017\n",
      "batch_n 120: SGD lr 0.0016 -> 0.0016\n",
      "batch_n 180: SGD lr 0.0016 -> 0.0016\n",
      "Epoch 13/20, Average Loss: 4.7527\n",
      "Time taken for one epoch: 8.152844905853271 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] peter will meet at the cinema at the weekend [EOS]\n",
      "\n",
      "batch_n 60: SGD lr 0.0016 -> 0.0016\n",
      "batch_n 120: SGD lr 0.0016 -> 0.0016\n",
      "batch_n 180: SGD lr 0.0016 -> 0.0016\n",
      "Epoch 14/20, Average Loss: 4.6956\n",
      "Time taken for one epoch: 8.186370134353638 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] it [EOS]\n",
      "\n",
      "batch_n 60: SGD lr 0.0015 -> 0.0015\n",
      "batch_n 120: SGD lr 0.0015 -> 0.0015\n",
      "batch_n 180: SGD lr 0.0015 -> 0.0015\n",
      "Epoch 15/20, Average Loss: 4.6450\n",
      "Time taken for one epoch: 8.376615285873413 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] nancy ' t have a link to the link to the weekend [EOS]\n",
      "\n",
      "batch_n 60: SGD lr 0.0015 -> 0.0015\n",
      "batch_n 120: SGD lr 0.0015 -> 0.0015\n",
      "batch_n 180: SGD lr 0.0015 -> 0.0015\n",
      "Epoch 16/20, Average Loss: 4.5953\n",
      "Time taken for one epoch: 8.287584066390991 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] there are going to go to go to talk to talk to talk to talk to talk to talk to talk to talk to talk to talk to talk to talk to talk to talk to talk to talk to talk to talk to talk to talk to the cinema\n",
      "\n",
      "batch_n 60: SGD lr 0.0014 -> 0.0014\n",
      "batch_n 120: SGD lr 0.0014 -> 0.0014\n",
      "batch_n 180: SGD lr 0.0014 -> 0.0014\n",
      "Epoch 17/20, Average Loss: 4.5496\n",
      "Time taken for one epoch: 8.182178974151611 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] peter will be there will be there will be there will be there [EOS]\n",
      "\n",
      "batch_n 60: SGD lr 0.0014 -> 0.0014\n",
      "batch_n 120: SGD lr 0.0014 -> 0.0014\n",
      "batch_n 180: SGD lr 0.0014 -> 0.0014\n",
      "Epoch 18/20, Average Loss: 4.5087\n",
      "Time taken for one epoch: 8.213618516921997 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] mary will be home in the new york city centre in the office [EOS]\n",
      "\n",
      "batch_n 60: SGD lr 0.0014 -> 0.0014\n",
      "batch_n 120: SGD lr 0.0014 -> 0.0014\n",
      "batch_n 180: SGD lr 0.0013 -> 0.0013\n",
      "Epoch 19/20, Average Loss: 4.4719\n",
      "Time taken for one epoch: 8.183064699172974 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] john is not be there in the library she is going to meet at the library and he will be there [EOS]\n",
      "\n",
      "batch_n 60: SGD lr 0.0013 -> 0.0013\n",
      "batch_n 120: SGD lr 0.0013 -> 0.0013\n",
      "batch_n 180: SGD lr 0.0013 -> 0.0013\n",
      "Epoch 20/20, Average Loss: 4.4362\n",
      "Time taken for one epoch: 8.193113088607788 sec\n",
      "Example summarization on the test set:\n",
      "  True summarization:\n",
      "    [SOS] hannah needs betty's number but amanda doesn't have it. she needs to contact larry. [EOS]\n",
      "  Predicted summarization:\n",
      "    [SOS] john and she ' t have to the new one of the next to the end of the station [EOS]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if MPS is available\n",
    "\n",
    "\n",
    "num_epochs = 20\n",
    "# learning_rate = 0.0001\n",
    "\n",
    "# Define the model parameters\n",
    "num_layers = 2\n",
    "embedding_dim = 128\n",
    "kdim = vdim = embedding_dim\n",
    "fully_connected_dim = 128\n",
    "num_heads = 2\n",
    "positional_encoding_length = 256\n",
    "\n",
    "# Initialize the model\n",
    "transformer = Transformer(\n",
    "    num_layers, \n",
    "    embedding_dim, \n",
    "    num_heads,\n",
    "    kdim,\n",
    "    vdim,\n",
    "    fully_connected_dim,\n",
    "    vocab_size, \n",
    "    vocab_size, \n",
    "    positional_encoding_length, \n",
    "    positional_encoding_length,\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Assuming 0 is the padding index\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0002, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = CustomSchedule(optimizer, d_model=embedding_dim)\n",
    "\n",
    "train_losses = []\n",
    "epoch_losses = []\n",
    "\n",
    "print_every = 60\n",
    "\n",
    "# Before training loop\n",
    "# for name, param in transformer.named_parameters():\n",
    "#     print(f\"{name}: requires_grad = {param.requires_grad}\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    transformer.train()\n",
    "    total_loss = 0\n",
    "    batch_count = 0\n",
    "    start = time.time()\n",
    "    \n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        src, tgt = batch\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "        \n",
    "        # Prepare input and target sequences\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        tgt_output = tgt[:, 1:]\n",
    "        \n",
    "        # Create masks\n",
    "        enc_padding_mask = create_padding_mask(src)\n",
    "        enc_padding_mask = enc_padding_mask.to(device)\n",
    "        look_ahead_mask = create_look_ahead_mask(tgt_input.shape[0], num_heads, tgt_input.size(1))\n",
    "        look_ahead_mask = look_ahead_mask.to(device)\n",
    "        dec_padding_mask = create_padding_mask(src)  # Notice that both encoder and decoder padding masks are equal\n",
    "        dec_padding_mask = dec_padding_mask.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        output, _ = transformer(src, tgt_input, enc_padding_mask, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        # # Check model output\n",
    "        # print(f\"Model output shape: {output.shape}\")\n",
    "        # print(f\"Model output min: {output.min()}, max: {output.max()}, mean: {output.mean()}\")\n",
    "\n",
    "        # # Check target\n",
    "        # print(f\"Target shape: {tgt_output.shape}\")\n",
    "        # print(f\"Target min: {tgt_output.min()}, max: {tgt_output.max()}\")\n",
    "\n",
    "        # output_id = torch.argmax(output, dim=-1)\n",
    "        # Reshape output and target for loss computation\n",
    "        output = output.reshape(-1, vocab_size)\n",
    "        tgt_output = tgt_output.reshape(-1).to(torch.long)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(output, tgt_output)\n",
    "\n",
    "        # Check loss computation\n",
    "        # print(f\"Computed loss: {loss.item()}\")\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "\n",
    "        # scheduler.step_and_update_lr()\n",
    "\n",
    "        before_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        after_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        \n",
    "        # # Inside training loop, after loss.backward()\n",
    "        # print(f\"Loss: {loss.item()}\")\n",
    "        # for name, param in transformer.named_parameters():\n",
    "        #     if param.grad is None:\n",
    "        #         print(f\"No grad for {name}\")\n",
    "        #     elif param.grad.sum() == 0:\n",
    "        #         print(f\"Zero grad for {name}\")\n",
    "        #     else:\n",
    "        #         print(f\"Grad for {name}: min = {param.grad.min()}, max = {param.grad.max()}, mean = {param.grad.mean()}\")\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        # train_losses.append(loss.item())\n",
    "        batch_count += 1\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        # Print loss every N batches\n",
    "        if (batch_idx + 1) % print_every == 0:\n",
    "            print(\"batch_n %d: SGD lr %.4f -> %.4f\" % (batch_count, before_lr, after_lr))\n",
    "        #     avg_loss = total_loss / batch_count\n",
    "        #     print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx+1}/{len(dataloader)}, Loss: {avg_loss:.4f}\")\n",
    "        #     total_loss = 0\n",
    "        #     batch_count = 0\n",
    "\n",
    "        # if batch_idx % 100 == 0:\n",
    "        #     current_lr = scheduler._optimizer.param_groups[0]['lr']\n",
    "        #     print(f'Train Step: {batch_idx}, Loss: {loss.item()}, Learning Rate: {current_lr}')\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    epoch_losses.append(avg_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    print (f'Time taken for one epoch: {time.time() - start} sec')\n",
    "    print('Example summarization on the test set:')\n",
    "    print('  True summarization:')\n",
    "    print(f'    {true_summary}')\n",
    "    print('  Predicted summarization:')\n",
    "    print(f'    {summarize(transformer, true_document)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35687ddc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Plot the loss funtion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eb3d5335",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Epoch')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2f564ba2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Epoch')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epoch_losses)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a53f16",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<a name='13'></a>\n",
    "# 13 - Summarize some Sentences!\n",
    "\n",
    "Below you can see an example of summarization of a sentence from the training set and a sentence from the test set. See if you notice anything interesting about them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2493b755",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set example:\n",
      "[SOS] amanda: i baked  cookies. do you want some?  jerry: sure!  amanda: i'll bring you tomorrow :-) [EOS]\n",
      "\n",
      "Human written summary:\n",
      "[SOS] amanda baked cookies and will bring jerry some tomorrow. [EOS]\n",
      "\n",
      "Model written summary:\n",
      "[SOS] tom and he will be late [EOS]\n"
     ]
    }
   ],
   "source": [
    "training_set_example = 0\n",
    "\n",
    "# Check a summary of a document from the training set\n",
    "print('Training set example:')\n",
    "print(document[training_set_example])\n",
    "print('\\nHuman written summary:')\n",
    "print(summary[training_set_example])\n",
    "print('\\nModel written summary:')\n",
    "print(summarize(transformer, document[training_set_example]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "15baaa47",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set example:\n",
      "[SOS] will: hey babe, what do you want for dinner tonight?  emma:  gah, don't even worry about it tonight  will: what do you mean? everything ok?  emma: not really, but it's ok, don't worry about cooking though, i'm not hungry  will: well what time will you be home?  emma: soon, hopefully  will: you sure? maybe you want me to pick you up?  emma: no no it's alright. i'll be home soon, i'll tell you when i get home.   will: alright, love you.   emma: love you too.  [EOS]\n",
      "\n",
      "Human written summary:\n",
      "[SOS] emma will be home soon and she will let will know. [EOS]\n",
      "\n",
      "Model written summary:\n",
      "[SOS] it ' s place to the party [EOS]\n"
     ]
    }
   ],
   "source": [
    "test_set_example = 3\n",
    "\n",
    "# Check a summary of a document from the test set\n",
    "print('Test set example:')\n",
    "print(document_test[test_set_example])\n",
    "print('\\nHuman written summary:')\n",
    "print(summary_test[test_set_example])\n",
    "print('\\nModel written summary:')\n",
    "print(summarize(transformer, document_test[test_set_example]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebd7ef5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "If you critically examine the output of the model, you can notice a few things:\n",
    " - In the training set the model output is (almost) identical to the real output (already after 20 epochs and even more so with more epochs). This might be because the training set is relatively small and the model is relatively big and has thus learned the sentences in the training set by heart (overfitting).\n",
    " - While the performance on the training set looks amazing, it is not so good on the test set. The model overfits, but fails to generalize. Again an easy candidate to blame is the small training set and a comparatively large model, but there might be a variety of other factors.\n",
    " - Look at the test set example 3 and its summarization. Would you summarize it the same way as it is written here? Sometimes the data may be ambiguous. And the training of **your model can only be as good as your data**.\n",
    "\n",
    "Here you only use a small dataset, to show that something can be learned in a reasonable amount of time in a relatively small environment. Generally, large transformers are trained on more than one task and on very large quantities of data to achieve superb performance. You will learn more about this in the rest of this course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41014aac",
   "metadata": {},
   "source": [
    "**Congratulations on finishing this week's assignment!** You did a lot of work and now you should have a better understanding of the Transformers and their building blocks (encoder and decoder) and how they can be used for text summarization. And remember: you dont need to change much to use the same model for a translator, just change the dataset and it should work!\n",
    "\n",
    "**Keep it up!**"
   ]
  }
 ],
 "metadata": {
  "grader_version": "1",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
